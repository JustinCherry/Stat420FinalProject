---
title: "Final Project"
author: "Bryan Winmill - winmill2 AND Justin Cherry - wcherry2"
date: "7/17/2019"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

### Introduction

College basketball is a huge sport. Every season thousands of fans watch in anticipation as hundreds of schools compete face to face on the court for basketball glory. Every step along the way those fans are dying to know "how will our team do this season?" 

So what if there was a way to predict this? What if we were able to analyze some statistics and form an accurate predition of how well a team would perform? We would be able to answer every fan's lingering question: "How will we do this season?"

This is what we'll explore in this project. We will look at the data and see if there's a way to accurately predict a team's performance.

**The Data:**

- For this project, we're going to use the dataset found at https://www.sports-reference.com/cbb/schools/  which includes data from 479 College basketball teams that were classified as a Division I school (or equivalent) for at least one season from 1893 to 2019. We've made slight alterations to the original dataset including renaming "W-L%" to just "WL" and separating "City,State" into "City" and "State".

**The Variables:**
<div class="col3">
- **Rank:** Rank of School
- **School:** Name of School
- **City:** School location (categorical predictor)
- **State:** School location (categorical predictor)
- **From:** First year classified as Division I (categorical predictor
- **To:** Last year classified as Division I (categorical predictor)
- **Yrs:** Number of years played
- **G:** Total Games played
- **W:** Total Games won
- **L:** Total Games lost
- **WL** Win-Loss percentage (This will be our response variable)
- **SRS:** (Simple Rating System): A rating that takes into account average point differential and strength of schedule. The rating is denominated in points above/below average, where zero is average. 
- **SOS:** (Strength of Schedule): A rating of strength of schedule. The rating is denominated in points above/below average, where zero is average. 
- **AP:**  Number of schools ranked in final AP poll (poll taken before the conclusion of the NCAA tournament)
- **CREG:** Number of regular season conference championships won
- **CTRN:** Number of conference tournament championships won
- **NCAA:** Number of NCAA Tournament appearances
- **FF:** Number of NCAA Final Four appearances
- **NC:** Number of NCAA Tournament championships won
</div>


### Methods

The methods section should contain the bulk of your “work.” This section will contain the bulk of the R code that is used to generate the results. Your R code is not expected to be perfect idiomatic R, but it is expected to be understood by a reader without too much effort. Use RMarkdown and code comments to your advantage to explain your code if needed.

This section should contain any information about data preparation that is performed to the original data before modelling. Then you will apply methods seen in class, which may include some of the following but are not limited to:

    Multiple linear regression
    ANOVA
    Dummy variables
    Interaction
    Residual diagnostics
    Outlier diagnostics
    Transformations
    Polynomial regression
    Model selection

Your task is not to use as many methods as possible. Your task is to use appropriate methods to find a good model that can correctly answer a question about the dataset, and then to communicate your result effectively.

```{r message=FALSE}
library(readr)
library(lmtest)
schoolData = read_csv("SchoolData.csv")
schoolData = na.omit(schoolData)
schoolData$State = as.factor(schoolData$State)
#schoolData

set.seed(420)
school_trn_idx  = sample(nrow(schoolData), size = trunc(0.80 * nrow(schoolData)))
school_trn_data = schoolData[school_trn_idx, ]
school_tst_data = schoolData[-school_trn_idx, ]

null_model = lm(WL ~ 1, data = school_trn_data)

full_model = lm(WL ~ . - (School + City), data = school_trn_data)

start_model = lm(WL ~ 1, data = school_trn_data)
forward_BIC = step(start_model, scope = WL ~ State + From + To + Yrs + G + SRS + SOS + AP + CREG + CTRN + NCAA + FF + NC, direction = "forward", trace = 0)
#forward_BIC

modified_forward_BIC = lm(WL ~ I(SRS ^ 2) + SOS + I(To ^ 2) + From + Yrs + CTRN + I(AP ^ 2) + I(CREG ^ 2) + I(NC ^ 2), data = school_trn_data)

```

```{r}
diagnostics = function(model, pcol = 'dodgerblue', lcol = 'darkorange', alpha = 0.05, plotit = TRUE, testit = TRUE)
{
  if(plotit == TRUE)
  {
    par(mfrow = c(1, 2))
    plot(fitted(model), resid(model), 
         col = pcol, 
         xlab = "Fitted", 
         ylab = "Residuals", 
         main = "Fitted vs Residuals Plot")
    abline(h = 0, col = lcol)
    
    qqnorm(resid(model), col = pcol, main = "QQ Plot")
    qqline(resid(model), col = lcol)
  }
  
  if(testit == TRUE)
  {
    p_val = shapiro.test(resid(model))$p.value
    decision = ifelse(p_val < alpha, "Reject", "Fail to Reject")
    list(p_val = p_val, decision = decision)
  }
  
}
```

### Results
The results section should contain numerical or graphical summaries of your results. You should report a final model you have chosen. There is not necessarily one, singular correct model, but certainly some methods and models are better than others in certain situations. You may use any methods we studied this semester to complete this task and provide evidence that your final choice of model is a good one.

- The model we decided was best for our test was the model using From, To, Yrs, SRS, SOS, CTRN, AP, CREG, and NC as the predictors (Also included below). 

```{r}
modified_forward_BIC = lm(WL ~ I(SRS ^ 2) + SOS + I(To ^ 2) + From + Yrs + CTRN + I(AP ^ 2) + I(CREG ^ 2) + I(NC ^ 2), data = school_trn_data)
```

- Below you can see the results of some of the tests we performed.

```{r}
diagnostics(modified_forward_BIC)
```

- Also, we compared the predicted vs actual data points to see how accurate/well our model did.

```{r}
pred_vals = predict(modified_forward_BIC, newdata = school_tst_data)
act_vals = school_tst_data$WL

avg_percent_error = mean((abs(pred_vals -act_vals))/pred_vals)*100

plot(act_vals ~ pred_vals,col = 'dodgerblue',
     xlab = "Predicted Values",
     ylab = "Actual Values")
abline(0,1, col = "darkorange")
```

- This graph above is very generic... So we wanted to show more personalized results. 

- Say someone going to Brigham Young University (Provo) wanted to see a prediction for about how well (What will be their Win-Loss ratio) the Cougars will do in a season. Having the following information - SRS = 7.56, SOS = 3.54, To = 2019, From = 1903, Yrs = 117, CTRN = 3, AP = 10, CREG = 25, NC = 0... We could make a prediction for what their Win-Loss ratio will be.

```{r}
prediction = data.frame(SRS = 7.56,
                        SOS = 3.54,
                        To = 2019,
                        From = 1903,
                        Yrs = 117,
                        CTRN = 3,
                        AP = 10,
                        CREG = 25,
                        NC = 0
                        )
predicted = predict(modified_forward_BIC, newdata = prediction)
actual = schoolData[35, 11]
```

- As you can see, with that information the predicted Win-Loss ration was `r predicted` and the actual Win-Loss ratio was `r actual`. The prediction was off by `r actual-predicted` which is pretty close. The Cougar fan would then know that their team will probably win a little more than half of their games.


### Discussion
The discussion section should contain discussion of your results. This should frame your results in the context of the data. How is your final model useful?

- After analyzing the different variables, we came to the conclusion that the best model was using From, To, Yrs, SRS, SOS, CTRN, AP, CREG, and NC as the predictors (some of them considering interactions). These variables give information including when a team started playing in the seasons, how long they have been playing, the strength of their schedule (how difficult the teams are they were playing), and even some of the results in championships (whether they won or lost). Using these variables we were able to create a model and create a graph that would show how close we could predict a teams Win Loss ration based on these predictors. Looking at the graph you can see that the predictions aren't perfect, but they can give a pretty good guess as to what a teams win loss ration will be. Just by looking at the graph you can see that with those predictors, it was able to guess within about 10% their win loss ratios. I feel like this is pretty good considering all the other models we looked at and how bad they seemed to be.

- Also, we threw in a Fitted vs Residuals model and a QQ plot. Both of these had the best results with the model we selected after breaking it down to the best version. While it still could be up for consideration whether the assumptions are violated, it is definitely better than the other models we had analyzed. We were able to get the p-value of the shapiro test above 0.05 which also was a good reason for the decision "fail to reject".

### Appendix 

<style>
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>

The appendix section should contain code and analysis that is used, but that may clutter the report or is not directly related to the choice of mode

```{r, eval = FALSE}

# These are some models we tested with, but after using the diagnostics and anova, we found that they were not very good models.

full_model = lm(WL ~ . - (School + City), data = school_trn_data)

significant_model = lm(WL ~ To + SRS + SOS + AP + NCAA, data = school_trn_data)

backward_AIC = step(full_model, direction = "backward", trace = 0)

n = length(full_model$residuals)
backward_BIC = step(full_model, direction = "backward", k = log(n), trace = 0)

#This was useful when determining our final model, but not useful as a test afterward. We chose to include here for reference.
#influential points
big_mod_cd = cooks.distance(modified_forward_BIC)
sum(big_mod_cd > 4 / length(modified_forward_BIC))
```

### Rubric - TODO: Remove before submission

DUE: Sat, 8/3/2019
Introduction (10 pts): This section fully provides background to your work such that a reader understands the purpose and goal of the data analysis project.

Methods (20 pts): This section will contain the bulk of the modeling decisions and R code that is used to generate the results. Have you used the appropriate methods for your dataset? Have you applied them correctly?

Results (15 pts): This section contains numerical or graphical summaries of your results as they pertain to the goal of the project. Do you arrive at the correct statistical conclusions from the analyses you perform? Did you make appropriate adjustments to your model selection and methods based on your results?

Discussion (15 pts): This section provides a full discussion of the results. It should include your observations and analysis that tie the Introduction, Methods, and Results together. The discussion should address the primary statistical themes of the analysis and your discoveries, and your interpretation of the results. Do you discuss your analysis results in the context of the data?

RMarkdown Usage and Programming Efficacy (15 pts): The Rmd file performs the required tasks in an efficient method and is fully correct. The simulated data is created, read, and accessed according to best practices. The R and RMarkdown usage address the needs of the Methods section and produces what is displayed in the Results section. Does your code perform the desired task? Is your code readable?

Neatness and Organization of the Report and Rmd files (15 pts): The Rmd file and Report are written and presented in a conventional style using appropriate spacing that is easy to read. The Report attempts to answer what was asked for in the directions and addressed in the Introduction. Plots are easily readable and self-explanatory. R output is limited to what is necessary to directly answer the exercise. Is your report easy to read? Is it written in a manner such that a reader does not already need to be familiar with the data?