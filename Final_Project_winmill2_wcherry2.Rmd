---
title: "Final Project"
author: "Bryan Winmill - winmill2 AND Justin Cherry - wcherry2"
date: "7/17/2019"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

# Final Project


### Introduction

College basketball is a huge sport. Every season thousands of fans watch in anticipation as hundreds of schools compete face to face on the court for basketball glory. Every step along the way those fans are dying to know "how will our team do this season?" 

So what if there was a way to predict this? What if we were able to analyze some statistics and form an accurate predition of how well a team would perform? We would be able to answer every fan's lingering question: "How will we do this season?"

This is what we'll explore in this project. We will look at the data and see if there's a way to accurately predict a team's performance.


**The Data:**

- For this project, we're going to use the dataset found at https://www.sports-reference.com/cbb/schools/  which includes data from 479 College basketball teams that were classified as a Division I school (or equivalent) for at least one season from 1893 to 2019. We've made slight alterations to the original dataset including renaming "W-L%" to just "WL" and separating "City,State" into "City" and "State".

**The Variables:**
<div class="col3">
- **Rank:** Rank of School
- **School:** Name of School
- **City:** School location (categorical predictor)
- **State:** School location (categorical predictor)
- **From:** First year classified as Division I (categorical predictor
- **To:** Last year classified as Division I (categorical predictor)
- **Yrs:** Number of years played
- **G:** Total Games played
- **W:** Total Games won
- **L:** Total Games lost
- **WL** Win-Loss percentage (This will be our response variable)
- **SRS:** (Simple Rating System): A rating that takes into account average point differential and strength of schedule. The rating is denominated in points above/below average, where zero is average. 
- **SOS:** (Strength of Schedule): A rating of strength of schedule. The rating is denominated in points above/below average, where zero is average. 
- **AP:**  Number of schools ranked in final AP poll (poll taken before the conclusion of the NCAA tournament)
- **CREG:** Number of regular season conference championships won
- **CTRN:** Number of conference tournament championships won
- **NCAA:** Number of NCAA Tournament appearances
- **FF:** Number of NCAA Final Four appearances
- **NC:** Number of NCAA Tournament championships won
</div>




The introduction section should relay what you are attempting to accomplish. It should provide enough background to your work such that a reader would not need to load your data to understand your report. Like the simulation project, you can assume the reader is familiar with the course concepts, but not your data. Some things to consider:

    What is this data? Where did it come from? What are the variables? Why is it interesting to you?
    Why are you creating a model for this data? What is the goal of this model?
    
- TODO: Write a good explanation.

### Methods

The methods section should contain the bulk of your “work.” This section will contain the bulk of the R code that is used to generate the results. Your R code is not expected to be perfect idiomatic R, but it is expected to be understood by a reader without too much effort. Use RMarkdown and code comments to your advantage to explain your code if needed.

This section should contain any information about data preparation that is performed to the original data before modelling. Then you will apply methods seen in class, which may include some of the following but are not limited to:

    Multiple linear regression
    ANOVA
    Dummy variables
    Interaction
    Residual diagnostics
    Outlier diagnostics
    Transformations
    Polynomial regression
    Model selection

Your task is not to use as many methods as possible. Your task is to use appropriate methods to find a good model that can correctly answer a question about the dataset, and then to communicate your result effectively.

```{r message=FALSE}
library(readr)
library(lmtest)
schoolData = read_csv("SchoolData.csv")
schoolData = na.omit(schoolData)
schoolData$State = as.factor(schoolData$State)
#schoolData

set.seed(420)
school_trn_idx  = sample(nrow(schoolData), size = trunc(0.80 * nrow(schoolData)))
school_trn_data = schoolData[school_trn_idx, ]
school_tst_data = schoolData[-school_trn_idx, ]

null_model = lm(WL ~ 1, data = school_trn_data)

full_model = lm(WL ~ . - (School + City), data = school_trn_data)

start_model = lm(WL ~ 1, data = school_trn_data)
forward_BIC = step(start_model, scope = WL ~ State + From + To + Yrs + G + SRS + SOS + AP + CREG + CTRN + NCAA + FF + NC, direction = "forward", trace = 0)
#forward_BIC

modified_forward_BIC = lm(WL ~ I(SRS ^ 2) + SOS + I(To ^ 2) + From + Yrs + CTRN + I(AP ^ 2) + I(CREG ^ 2) + I(NC ^ 2), data = school_trn_data)

```

```{r}
diagnostics = function(model, pcol = 'dodgerblue', lcol = 'darkorange', alpha = 0.05, plotit = TRUE, testit = TRUE)
{
  if(plotit == TRUE)
  {
    par(mfrow = c(1, 2))
    plot(fitted(model), resid(model), 
         col = pcol, 
         xlab = "Fitted", 
         ylab = "Residuals", 
         main = "Fitted vs Residuals Plot")
    abline(h = 0, col = lcol)
    
    qqnorm(resid(model), col = pcol, main = "QQ Plot")
    qqline(resid(model), col = lcol)
  }
  
  if(testit == TRUE)
  {
    p_val = shapiro.test(resid(model))$p.value
    decision = ifelse(p_val < alpha, "Reject", "Fail to Reject")
    list(p_val = p_val, decision = decision)
  }
  
}

### SHAPIRO TEST WE WANT A LARGE P-VALUE (NO violation with the normality assumption)
### BP TEST WE WANT A LARGE P-VALUE (NO violation with the constant variance assumption)
diagnostics(modified_forward_BIC)
```


```{r message=FALSE}
anova(null_model, modified_forward_BIC)
```

- TODO: Test model

- TODO: Anova to show model we are using is best ----- DID SOME WORK

- TODO: Check residuals/outliers/how much leverage

- TODO: LOOK AT BELOW OPTIONS
- The null hypothesis of the test
- The test statistic of the test
- The p-value of the test
- A statistical decision
- A practical conclusion

### Results
The results section should contain numerical or graphical summaries of your results. You should report a final model you have chosen. There is not necessarily one, singular correct model, but certainly some methods and models are better than others in certain situations. You may use any methods we studied this semester to complete this task and provide evidence that your final choice of model is a good one.


- TODO: Plot results of diagnostics/evidence (ANOVA)

- TODO: Plot model and/or data (PLOT/HIST/SCAT)

- TODO: Create table (possibly)

### Discussion
The discussion section should contain discussion of your results. This should frame your results in the context of the data. How is your final model useful?

- TODO: Write a good discussion

### Appendix 

<style>
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>

The appendix section should contain code and analysis that is used, but that may clutter the report or is not directly related to the choice of mode

```{r, eval = FALSE}

# These are some models we tested with, but after using the diagnostics and anova, we found that they were not very good models.

full_model = lm(WL ~ . - (School + City), data = school_trn_data)

significant_model = lm(WL ~ To + SRS + SOS + AP + NCAA, data = school_trn_data)

backward_AIC = step(full_model, direction = "backward", trace = 0)

n = length(full_model$residuals)
backward_BIC = step(full_model, direction = "backward", k = log(n), trace = 0)
```




### Rubric - TODO: Remove before submission

DUE: Sat, 8/3/2019
Introduction (10 pts): This section fully provides background to your work such that a reader understands the purpose and goal of the data analysis project.

Methods (20 pts): This section will contain the bulk of the modeling decisions and R code that is used to generate the results. Have you used the appropriate methods for your dataset? Have you applied them correctly?

Results (15 pts): This section contains numerical or graphical summaries of your results as they pertain to the goal of the project. Do you arrive at the correct statistical conclusions from the analyses you perform? Did you make appropriate adjustments to your model selection and methods based on your results?

Discussion (15 pts): This section provides a full discussion of the results. It should include your observations and analysis that tie the Introduction, Methods, and Results together. The discussion should address the primary statistical themes of the analysis and your discoveries, and your interpretation of the results. Do you discuss your analysis results in the context of the data?

RMarkdown Usage and Programming Efficacy (15 pts): The Rmd file performs the required tasks in an efficient method and is fully correct. The simulated data is created, read, and accessed according to best practices. The R and RMarkdown usage address the needs of the Methods section and produces what is displayed in the Results section. Does your code perform the desired task? Is your code readable?

Neatness and Organization of the Report and Rmd files (15 pts): The Rmd file and Report are written and presented in a conventional style using appropriate spacing that is easy to read. The Report attempts to answer what was asked for in the directions and addressed in the Introduction. Plots are easily readable and self-explanatory. R output is limited to what is necessary to directly answer the exercise. Is your report easy to read? Is it written in a manner such that a reader does not already need to be familiar with the data?

